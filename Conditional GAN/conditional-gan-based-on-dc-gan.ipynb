{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from matplotlib import image\nimport torch\nimport torch.nn as nn\n\nimport numpy as np\n\nimport torch.optim as optim\nimport torchvision\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\nimport torchvision.utils as vutils\n\nclass Discriminator(nn.Module):\n    def __init__(self, img_channels, features_d,num_classes,image_size):\n        super(Discriminator, self).__init__()\n        self.image_size = image_size\n        self.disc = nn.Sequential(\n            # input: N x img_channels x 64 x 64\n            nn.Conv2d(\n                img_channels+1, features_d, kernel_size=4, stride=2, padding=1\n                #image_channels+1 for embedding layer of labels\n            ), #32 x 32\n            nn.LeakyReLU(0.2),\n\n            self._block(features_d, features_d * 2, 4, 2, 1), #16 x16\n            self._block(features_d * 2, features_d * 4, 4, 2, 1), #8 x 8\n            self._block(features_d * 4, features_d * 8, 4, 2, 1), #4 x 4\n           \n            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n            nn.Sigmoid(),\n        )\n\n        self.embed = nn.Embedding(num_classes, image_size*image_size)\n\n    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n        return nn.Sequential(\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size,\n                stride,\n                padding,\n                bias=False,\n            ),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(0.2),\n        )\n\n    def forward(self, x, labels):\n        embedding = (self.embed(labels)).view(labels.shape[0], 1, self.image_size, self.image_size)\n        x = torch.cat([x, embedding], dim=1)\n        return self.disc(x)\n\n\nclass Generator(nn.Module):\n    def __init__(self, z_dim, img_channels, features_g, image_size, num_classes, embedding_size):\n        super(Generator, self).__init__()\n        self.image_size = image_size\n        self.net = nn.Sequential(\n            # Input: N x z_dim x 1 x 1\n            self._block(z_dim+embedding_size, features_g * 16, 4, 1, 0),  # img: 4x4\n            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n            nn.ConvTranspose2d(\n                features_g * 2, img_channels, kernel_size=4, stride=2, padding=1\n            ),\n            # Output: N x img_channels x 64 x 64\n            nn.Tanh(),\n        )\n\n        self.embed = nn.Embedding(num_classes, embedding_size)\n\n    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n        return nn.Sequential(\n            nn.ConvTranspose2d(\n                in_channels,\n                out_channels,\n                kernel_size,\n                stride,\n                padding,\n                bias=False,\n            ),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n\n    def forward(self, x,labels):\n        #latent vector z: N x z_dim x 1 x 1\n        embedding = self.embed(labels).unsqueeze(2).unsqueeze(3)\n        x = torch.cat([x, embedding], dim=1)\n        return self.net(x)\n\n\ndef initialize_weights(model):\n    # Initializes weights according to the DCGAN paper\n    for m in model.modules():\n        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n            nn.init.normal_(m.weight.data, 0.0, 0.02) # mean 0 std 0.02\n\ndef test():\n    N, in_channels, H, W = 8, 3, 64, 64\n    noise_dim = 100\n    x = torch.randn((N, in_channels, H, W))\n    disc = Discriminator(in_channels, 8)\n    assert disc(x).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n    gen = Generator(noise_dim, in_channels, 8)\n    z = torch.randn((N, noise_dim, 1, 1))\n    assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\"\n    print(\"All tests passed\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n    Train a DCGAN on MNIST.\n\"\"\"\n\n\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nLEARNING_RATE = 2e-4\nBATCH_SIZE = 128\nIMAGE_SIZE = 64\nCHANNELS_IMG = 1\nNUM_CLASSES =10\nGEN_EMBEDDING = 100\n\nZ_DIM = 100\nNUM_EPOCHS = 100\nFEATURES_DISC = 64\nFEATURES_GEN = 64\n\n\ntransforms = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        [0.5 for _ in range(CHANNELS_IMG)],[ 0.5 for _ in range(CHANNELS_IMG)]),\n\n]\n)\n\ndataset = datasets.MNIST(root=\"./data\", train=True, transform=transforms, download=True)\nloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n\ndisc = Discriminator(CHANNELS_IMG, FEATURES_DISC, NUM_CLASSES,IMAGE_SIZE).to(device)\ngen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN, IMAGE_SIZE, NUM_CLASSES,GEN_EMBEDDING).to(device)\n\ninitialize_weights(gen)\ninitialize_weights(disc)\n\nopt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\nopt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\ncriterion = nn.BCELoss()\n\nfixed_noise = torch.randn(32, Z_DIM, 1, 1).to(device)\n\nimg_list = []\n\ngen.train()\ndisc.train()\niters = 0\n\nfor epoch in range(NUM_EPOCHS):\n    for i, (imgs, labels) in enumerate(loader):\n        iters += 1\n        imgs = imgs.to(device)\n        labels = labels.to(device)\n        noise = torch.randn(BATCH_SIZE, Z_DIM, 1, 1).to(device)\n        # Train Discriminator\n        fake = gen(noise, labels)\n\n        real_labels = torch.ones(BATCH_SIZE, 1, 1, 1).to(device)\n        fake_labels = torch.zeros(BATCH_SIZE, 1, 1, 1).to(device)\n        real_outputs = disc(imgs,labels)\n        fake_outputs = disc(fake.detach(), labels) # detach to avoid backprop through generator\n        real_loss = criterion(real_outputs, torch.ones_like(real_outputs))\n        fake_loss = criterion(fake_outputs, torch.zeros_like(fake_outputs))\n        d_loss = (real_loss + fake_loss) / 2\n        disc.zero_grad()\n        d_loss.backward()\n        opt_disc.step()\n        # Train Generator\n        \n        \n        gen_outputs = disc(fake, labels)\n        g_loss = criterion(gen_outputs, torch.ones_like(gen_outputs))\n        gen.zero_grad()\n        g_loss.backward()\n        opt_gen.step()\n\n\n        if (iters % 200 == 0) or ((epoch == NUM_EPOCHS-1) and (i == len(loader)-1)):\n            with torch.no_grad():\n                faker = gen(noise, labels)\n            img_list.append(vutils.make_grid(faker, padding=2, normalize=True))\n\n\n\n        if i == 0:\n            print(\"Epoch: {}/{}\".format(epoch, NUM_EPOCHS))\n            print(\"Discriminator loss: {}\".format(d_loss))\n            print(\"Generator loss: {}\".format(g_loss))\n            # print(\"Real outputs: {}\".format(real_outputs))\n            # print(\"Fake outputs: {}\".format(fake_outputs))\n            # print(\"Real labels: {}\".format(real_labels))\n            # print(\"Fake labels: {}\".format(fake_labels))\n            # print(\"Generator output: {}\".format(gen(noise)))\n            # print(\"Discriminator output: {}\".format(disc(imgs)))\n            print(\"\\n\")\n\n\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())","metadata":{},"execution_count":null,"outputs":[]}]}